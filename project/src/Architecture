# Folder structure :


src
│
├── Dataset/
│   ├── dataset_project_iapr2025/
│   │   ├──test.csv
│   │   ├──train.csv
│   │   ├──test
│   │   │   ├──L1000757.JPG
│   │   │   └──L100XXXX.JPG
│   │   └──train
│   │       ├──L1000757.JPG
│   │       └──L100XXXX.JPG
│   ├── praline_clean
│   │   ├── Amandina/           # 1000x1000 transparent PNGs of Amandina
│   │   ├── Arabia/                # 1000x1000 transparent PNGs of chocolate
│   │   ├── .../                # 1000x1000 transparent PNGs of chocolate
│   │   ├── Triangolo/                # 1000x1000 transparent PNGs of chocolate
│   │   ├── MiscObjects/        # 1000x1000 transparent PNGs of clutter
│   │   └── Background/         # 6000x4000 background images (jpg/png)
│   └── synthetic_dataset
│       ├──XXX
│       │   ├──images
│       │   │   ├──train
│       │   │   │   ├──1000000.JPG
│       │   │   │   └──100XXXX.JPG
│       │   │   └──val
│       │   │       ├──1000000.JPG
│       │   │       └──100XXXX.JPG
│       │   ├──train.csv
│       │   ├──val.csv
│       │   └──recap.txt
│       └──YYY
│           ├──images
│           │   ├──train
│           │   │   ├──1000000.JPG
│           │   │   └──100XXXX.JPG
│           │   └──val
│           │       ├──1000000.JPG
│           │       └──100XXXX.JPG
│           ├──train.csv
│           ├──val.csv
│           └──recap.txt
├── checkpoints/
│   └── ZZZ/
│       ├──recap.txt // parameters recap of the run. (dataset used, arch used, checkpoint used, ...)
│       ├──loss.png // show the training loss + testing loss + F1 ferformance over epoch
│       ├──epoch_0.pt
│       ├──epoch_0.csv // inference on testing set
│       ├──epoch_0.txt // store the epoch performance metric (loss train + test, F1 validation + per class)
│       ├──epoch_N.pt
│       ├──epoch_N.csv // inference on testing set
│       └──epoch_N.txt // store the epoch performance metric (loss train + test, F1 validation + per class)
├── yoco_large.py/ -> 12M parameters
├── yoco_medium.py/ -> 1M parameters
├── yoco_small.py/ -> 200k parameters
├── yoco_tiny.py/ -> 50 k parameters
├──RUNME.py
└──yoco_trainer.py/



# RUNME.py
launched script, interactive shell tool to create the command for the other package.
scan the file structure to reduce the number of parameters that need to be tuned
look for epoch that have the best F1 score on testing dataset.

# yoco_trainer.py

TRAIN_IMG_DIR = Path("../chocolate_data/syntheticDataset") -> what to train on (synthetic)    (Loss)
TEST_IMG_DIR = Path("../chocolate_data/syntheticDataset") -> what to test on (synthetic)      (Loss)
TEST_TRAIN_IMG_DIR = Path("../chocolate_data/") -> what to VALIDATE on (original traning set) (F1)
TEST_VAL_IMG_DIR = Path("../chocolate_data/") -> what to VALIDATE on (original testing set)   (F1)

TRAIN_CSV = DATA_DIR / "train.csv" -> ground truth training synthetic
TEST_CSV = DATA_DIR / "val.csv" -> ground truth testing synthetic
TEST_TRAIN_CSV = DATA_DIR / "val.csv" -> ground truth testing synthetic
TEST_VAL_CSV = DATA_DIR / "val.csv" -> ground truth testing synthetic

YOCO_ARCH -> what model class to import (normal, big, small, tiny)
MODEL_PATH = "" -> if provided to not start from scratch.
OUTPUT_PATH = "" -> which folder to store the model after each epoch
-> store training stat (loss train + test, F1 validation + per class) (train + test original dataset) for each epoch -> make a single txt for each epoch
-> store each model epoch weight.
-> store provided parameters.
-> generate a png with the loss + F1 score over each epoch (in the same folder)
-> compute for each epoch the prediction.csv

BATCH_SIZE = 16
EPOCHS = 30
LEARNING_RATE = 1e-3
NUM_WORKER = 4 -> GPU vs CPU, windows linux dependent

# yoco.py (define as many as needed, keep input and output constant)

# synthethic dataset generator.py

# Configurable constants
NUM_IMAGES = 200
TRAIN_RATIO = 0.8

data_dir = Path("../chocolate_data/synthetic_dataset/XXX") -> where to store

BACKGROUND_DIR = Path("../chocolate_data/praline_clean/Background")
MISC_DIR = Path("../chocolate_data/praline_clean/MiscObjects")

PRALINE_DIRS = {
    "Jelly_White": Path("../chocolate_data/praline_clean/Jelly_White"),
    "Jelly_Milk": Path("../chocolate_data/praline_clean/Jelly_Milk"),
    "Jelly_Black": Path("../chocolate_data/praline_clean/Jelly_Black"),
    "Amandina": Path("../chocolate_data/praline_clean/Amandina"),
    "Crème_brulée": Path("../chocolate_data/praline_clean/Crème_brulée"),
    "Triangolo": Path("../chocolate_data/praline_clean/Triangolo"),
    "Tentation_noir": Path("../chocolate_data/praline_clean/Tentation_noir"),
    "Comtesse": Path("../chocolate_data/praline_clean/Comtesse"),
    "Noblesse": Path("../chocolate_data/praline_clean/Noblesse"),
    "Noir_authentique": Path("../chocolate_data/praline_clean/Noir_authentique"),
    "Passion_au_lait": Path("../chocolate_data/praline_clean/Passion_au_lait"),
    "Arabia": Path("../chocolate_data/praline_clean/Arabia"),
    "Stracciatella": Path("../chocolate_data/praline_clean/Stracciatella"),
}

RESCALE_FACTORS = {
    "Amandina": (1, 1),
    "Arabia": (0.7, 0.7),
    "Comtesse": (0.7, 0.7),
    "Crème_brulée": (0.5, 0.5),
    "Jelly_White": (0.5, 0.5),
    "Jelly_Milk": (0.5, 0.5),
    "Jelly_Black": (0.5, 0.5),
    "Noblesse": (0.7, 0.7),
    "Noir_authentique": (0.7, 0.7),
    "Passion_au_lait": (0.7, 0.7),
    "Stracciatella": (0.7, 0.7),
    "Tentation_noir": (0.7, 0.7),
    "Triangolo": (0.7, 0.7),
}

create a recap.txt in the folder that show the used argument


# main.py

load a model (or many)
run inference / prediction

Implement merging voting logic here if wanted

just compute the F1 score in training + validation dataset (class + global)
export a csv