{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f23e1c5",
   "metadata": {},
   "source": [
    "# [IAPR][iapr]: Final project - Chocolate Recognition\n",
    "\n",
    "\n",
    "**Moodle group ID:** *3*\n",
    "**Kaggle challenge:** *Deep learning*\n",
    "**Kaggle team name (exact):** \"*Byte the Bar*\"\n",
    "\n",
    "**Author 1 (sciper):** Nathann Morand (296190)\n",
    "\n",
    "**Author 2 (sciper):** David Croce (327277)\n",
    "\n",
    "**Author 3 (sciper):** Felipe Ramirez (331471)\n",
    "\n",
    "**Due date:** 21.05.2025 (11:59 pm)\n",
    "\n",
    "\n",
    "## Key Submission Guidelines:\n",
    "- **Before submitting your notebook, <span style=\"color:red;\">rerun</span> it from scratch!** Go to: `Kernel` > `Restart & Run All`\n",
    "- **Only groups of three will be accepted**, except in exceptional circumstances.\n",
    "\n",
    "\n",
    "[iapr]: https://github.com/LTS5/iapr2025\n",
    "\n",
    "---"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Introduction\n",
    "We are tasked to make a program that is able to count how many instance among 13 praline class in a cluttered image.\n",
    "We must retrain our model from scratch and are provided with only a very limited number of training image (90)\n",
    "The score is computed using a modified F1 score (that take difference in number of predicted praline)\n",
    "\n",
    "For our approach we chose to make convolutional model based of the yolo architecture but instead we rewrote the network head to directly predict the number of instance for each class. We named our architecture yoco : you only count once. To train it we chose to make a synthetic dataset generator based of cropped praline from the training dataset pasted on top of the empty background that where extracted."
   ],
   "id": "9fe63d4b00129735"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Dataset & Preprocessing\n",
    "The original dataset offer 90 image that are 6000x4000 px, .JPG The image where taken in similar lightning condition and are relatively well lit.\n",
    "The inference dataset has the same properties.\n",
    "\n",
    "## EDA\n",
    "Image from the dataset look like the following with different background object, different miscellaneous object scatter around and a few praline.\n",
    "<img src=\"chocolate_data/dataset_project_iapr2025/train/L1000957.JPG\" width=\"600\" height=\"400\"/>\n",
    "\n",
    "Using the provided CSV we computed the histogram of number of chocolate per image and the histogram showing the number of instance per class to see how well the class are balanced. We also show how many individual instance of praline are available across the dataset and the maximum number of chocolate of each class present on an image.\n"
   ],
   "id": "e3f98282e3030554"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load the CSV file\n",
    "df = pd.read_csv('chocolate_data/dataset_project_iapr2025/train.csv')\n",
    "\n",
    "# Calculate the total number of chocolates per image\n",
    "df['total_chocolates'] = df.iloc[:, 1:].sum(axis=1)\n",
    "\n",
    "# Print the total number of chocolates in the dataset\n",
    "total_chocolates_in_dataset = df['total_chocolates'].sum()\n",
    "print(f\"Total number of chocolates in the dataset: {total_chocolates_in_dataset}\")\n",
    "\n",
    "# Get the maximum number of instances per class\n",
    "max_per_class = df.iloc[:, 1:].max()\n",
    "\n",
    "# Print the results\n",
    "print(\"Maximum number of instances for each chocolate class in a single image:\")\n",
    "print(max_per_class)\n",
    "\n",
    "# Plot the histogram for total chocolates per image\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.hist(df['total_chocolates'], bins=range(df['total_chocolates'].min(), df['total_chocolates'].max() + 1), edgecolor='black')\n",
    "plt.title('Histogram of Total Chocolates per Image')\n",
    "plt.xlabel('Total Number of Chocolates')\n",
    "plt.ylabel('Frequency')\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Plot the histogram for class distribution (excluding total chocolates column)\n",
    "class_counts = df.iloc[:, 1:13].sum(axis=0)\n",
    "plt.figure(figsize=(12, 6))\n",
    "class_counts.plot(kind='bar', color='skyblue', edgecolor='black')\n",
    "plt.title('Class Balance Histogram')\n",
    "plt.xlabel('Chocolate Class')\n",
    "plt.ylabel('Number of Chocolates')\n",
    "plt.xticks(rotation=45)\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n"
   ],
   "id": "43b1082dfcccd78b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Instance extraction\n",
    "To make the synthetic dataset generator, we cropped manually the 583 praline present in the 90 image using a helper script to draw the box and save it in a new file. We made a second helper file to show the image and moving it to the correct folder after the operator write the class id thus making the sorting faster.\n",
    "\n",
    "Once the praline where cropped we spent many hours cleaning the background from the 584 pralines using paint or Gimp. That being done we made another helper script to re-orient, center and rescale the praline. The recalling factor allowed use to measure the size variation between the praline and thus know that the variation was +-20% and thus a single detection head would be sufficient. We also did the same with the misc object present and patched the hole in the background.\n",
    "\n",
    "Here are an overview of the cleaned praline :"
   ],
   "id": "e78cb5bd757d18e"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Define path and ignored folders\n",
    "base_path = 'chocolate_data/praline_clean'\n",
    "ignored_folders = {\"MiscObjects\", \"raw_praline\", \"references\", \"Background\"}\n",
    "\n",
    "# Get valid subfolders\n",
    "valid_folders = [f for f in os.listdir(base_path) if os.path.isdir(os.path.join(base_path, f)) and f not in ignored_folders]\n",
    "\n",
    "# Function to display a 6x6 image mosaic\n",
    "def display_mosaic(images, title):\n",
    "    fig, axes = plt.subplots(6, 6, figsize=(12, 12))\n",
    "    fig.suptitle(title, fontsize=16)\n",
    "    for i in range(36):\n",
    "        ax = axes[i // 6, i % 6]\n",
    "        if i < len(images):\n",
    "            ax.imshow(images[i])\n",
    "        ax.axis('off')\n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.96])\n",
    "    plt.show()\n",
    "\n",
    "# Process each valid folder\n",
    "for folder in valid_folders:\n",
    "    folder_path = os.path.join(base_path, folder)\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))]\n",
    "    image_files = image_files[:36]  # Limit to first 36 images\n",
    "\n",
    "    images = []\n",
    "    for img_file in image_files:\n",
    "        img_path = os.path.join(folder_path, img_file)\n",
    "        try:\n",
    "            img = Image.open(img_path).convert('RGB')\n",
    "            img = img.resize((200, 200))\n",
    "            images.append(img)\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading image {img_file}: {e}\")\n",
    "\n",
    "    display_mosaic(images, title=folder)\n"
   ],
   "id": "6d8fa65999ba045b",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Synthetic dataset generation\n",
    "\n",
    "To train our chocolate detection and counting model, we developed a synthetic dataset generator that creates realistic scenes by compositing high-quality, transparent PNG cutouts of pralines and clutter onto large photographic backgrounds. The generator is designed to mimic natural variations in object placement, orientation, scale, and density while ensuring dataset consistency and coverage across all 13 chocolate classes.\n",
    "\n",
    "#### Directory Structure\n",
    "\n",
    "The image assets are organized as follows:\n",
    "\n",
    "```\n",
    "../chocolate_data/\n",
    "├── praline_clean/\n",
    "│   ├── <ChocolateClass>/        # 1000x1000 transparent PNGs per class\n",
    "│   ├── MiscObjects/             # 1000x1000 PNGs of clutter (non-chocolates)\n",
    "│   └── Background/              # 6000x4000 high-res background images\n",
    "└── syntheticDataset/\n",
    "    ├── images/train/            # Generated training images\n",
    "    ├── images/val/              # Generated validation images\n",
    "    ├── train.csv                # YOLO-style count labels\n",
    "    └── val.csv\n",
    "```\n",
    "\n",
    "#### Scene Generation Logic\n",
    "\n",
    "For each synthetic scene, the generator performs the following steps:\n",
    "\n",
    "1. **Background Selection**: A random high-resolution background (6000×4000 px) is selected.\n",
    "\n",
    "2. **Misc Object Placement**:\n",
    "   - Randomly place 0–6 miscellaneous objects per image.\n",
    "   - Each object receives a random rotation (0–360°) and is scaled with ±20% jitter applied to base scale factors.\n",
    "   - Objects are not allowed to overlap but may touch. Up to 20 retry attempts are made to find valid positions.\n",
    "\n",
    "3. **Chocolate Placement**:\n",
    "   - Each of the 13 chocolate classes is assigned 0–5 instances per image based on a skewed probability distribution favoring 0 or 1.\n",
    "   - Each chocolate instance is rescaled (with class-specific base factors and jitter), rotated randomly, and placed while checking that overlaps do not exceed 20% with any existing chocolates (touching is allowed).\n",
    "   - At least one pair of chocolates (if more than two are present) is forced to touch to reflect realistic clutter.\n",
    "\n",
    "4. **Label Generation**:\n",
    "   - Labels are saved in CSV format compatible with YOLO count training, with each row representing a synthetic image and columns encoding the number of instances per class.\n",
    "   - Example:\n",
    "     ```\n",
    "     id,Jelly White,Jelly Milk,...,Stracciatella\n",
    "     1000001,2,1,...,0\n",
    "     ```\n",
    "\n",
    "5. **Scene Saving**:\n",
    "   - The final composite image can optionally be resized using a configurable downscaling factor.\n",
    "   - Image and corresponding label are saved in the appropriate `train` or `val` directory, based on a configurable split ratio (default: 80/20).\n",
    "\n",
    "#### Performance & Scalability\n",
    "\n",
    "- The generator uses multi-threading to parallelize image composition, utilizing `N-2` CPU cores to avoid overloading the system.\n",
    "- Progress is tracked using `tqdm` to provide live feedback.\n",
    "- The total number of generated scenes is configurable (default: 10,000), and all key parameters (e.g., scaling jitter, image size, split ratio) can be tuned easily.\n",
    "\n",
    "#### Result\n",
    "Using the technique descibed previously we could generate between 1000 and 20k picture similar to the following.\n",
    "<img src=\"chocolate_data/syntheticDataset/images/train/1000000.JPG\" width=\"600\" height=\"400\"/>\n",
    "\n"
   ],
   "id": "a0b1c303d43350fe"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Model Architecture\n",
    "\n",
    "YOCO Architecture – You Only Count Once\n",
    "\n",
    "### Objective\n",
    "The **YOCO** model is a custom convolutional neural network designed to **predict per-class object counts** in high-resolution images containing chocolate pralines. It avoids doing object detection + instance counting by directly classifying the number of instances for each of the 13 classes.\n",
    "\n",
    "\n",
    "### Model Architecture\n",
    "\n",
    "The YOCO network processes an RGB image of shape `(3, 800, 1200)` and outputs a tensor of shape `(13, 6)`, where:\n",
    "- 13 is the number of chocolate classes\n",
    "- 6 is the number of count classes: `[0, 1, 2, 3, 4, 5 or more]`\n",
    "Each entry is a **logit vector** representing the likelihood of that count for a given class.\n",
    "\n",
    "#### Feature Extractor\n",
    "\n",
    "| Layer | Details |\n",
    "|-------|---------|\n",
    "| Conv2d(3 → 16)  | Kernel=3×3, Stride=1, Padding=1 |\n",
    "| LeakyReLU(0.1)  | Non-linearity |\n",
    "| MaxPool2d       | 2×2 downsampling |\n",
    "| Conv2d(16 → 32) | Same pattern repeated |\n",
    "| Conv2d(32 → 64) | |\n",
    "| Conv2d(64 → 128) | |\n",
    "| Conv2d(128 → 256) | |\n",
    "| Conv2d(256 → 256) | Final feature map shape: **[B, 256, 7, 7]** |\n",
    "\n",
    "Unlike classical yolo which expect a square image and will pad it to fit here we can fit directly our image for the convolution after a simple rescaling while preserving aspect ratio. We choose to downscale the image by 4 as it would make the smallest praline ~50 px across which was deemed sufficient to keep the small feature still visible.\n",
    "#### Head\n",
    "\n",
    "| Layer | Details |\n",
    "|-------|---------|\n",
    "| Conv2d(256 → 128) | 3×3 conv with LeakyReLU(0.3) |\n",
    "| Conv2d(128 → 78)  | 1×1 conv (78 = 13 classes × 6 count bins) |\n",
    "| AdaptiveAvgPool2d | Global average pooling to [B, 78, 1, 1] |\n",
    "| Reshape           | Final output shape: **[B, 13, 6]** |\n",
    "\n",
    "These parameters just work, but it might have been possible to reduce the network size further with some tuning.\n",
    "We only have one head as the praline have roughly the same size and don't vary much across image which make a multiscale system necessary.\n",
    "\n",
    "### Output Format and Count Encoding\n",
    "\n",
    "For each class, the model predicts a **probability distribution over 6 count classes**:\n",
    "\n",
    "```\n",
    "0, 1, 2, 3, 4, 5+\n",
    "```\n",
    "\n",
    "> This is a **one-hot classification** over possible counts. For example, if there are exactly 3 pralines of class 5, the target vector is `[0, 0, 0, 1, 0, 0]` for that class.\n",
    "\n",
    "This encoding has multiple advantages:\n",
    "- It reflects the **discrete nature** of count prediction.\n",
    "- It's more robust than regression for small integer counts.\n",
    "- The `5+` bin handles the practical upper bound seen in the training data (no class had more than 5 pralines in a single image).\n",
    "\n",
    "---\n",
    "\n",
    "### Activation Function\n",
    "\n",
    "- **LeakyReLU** is used throughout the model:\n",
    "  - `LeakyReLU(0.1)` in the feature extractor\n",
    "  - `LeakyReLU(0.3)` in the head\n",
    "- Unlike ReLU, LeakyReLU allows a small gradient when the unit is not active, which helps with overfitting, **avoid dead neurons** and improves training stability.\n",
    "\n",
    "---\n",
    "\n",
    "### Loss Function\n",
    "\n",
    "The final loss is computed as the **average cross-entropy loss across all 13 classes**:\n",
    "\n",
    "```python\n",
    "loss = sum(criterion(logits[:, i], targets[:, i]) for i in range(NUM_CLASSES)) / NUM_CLASSES\n",
    "```\n",
    "\n",
    "Where:\n",
    "- `logits[:, i]` is the 6-dimensional output for class `i`\n",
    "- `targets[:, i]` is the ground-truth one-hot encoded target vector for class `i`\n",
    "- `criterion` is `nn.CrossEntropyLoss()`\n",
    "\n",
    "We used softmax activation because\n",
    "- Count values are **mutually exclusive**.\n",
    "- Softmax creates a valid **probability distribution** over the possible counts.\n",
    "- It's robust and simple to implement\n",
    "\n",
    "\n",
    "here is the full schematic of the architecture\n",
    "<img src=\"yoco_arch.png\" width=\"1000\" height=\"1000\"/>\n",
    "\n"
   ],
   "id": "ba5b1d66687464de"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-04-25T07:10:35.183347Z",
     "start_time": "2025-04-25T07:10:35.177537Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# the following snippet generate te network architecture image.\n",
    "\"\"\"\n",
    "from torchviz import make_dot\n",
    "import torch\n",
    "from src.yoco import YOCO\n",
    "model = YOCO()\n",
    "x = torch.randn(1, 3, 800, 1200)\n",
    "y = model(x)\n",
    "make_dot(y, params=dict(model.named_parameters())).render(\"yoco_arch\", format=\"png\")\n",
    "\"\"\""
   ],
   "id": "1cb33b0b82e550b9",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom torchviz import make_dot\\nimport torch\\nfrom src.yoco import YOCO\\nmodel = YOCO()\\nx = torch.randn(1, 3, 800, 1200)\\ny = model(x)\\nmake_dot(y, params=dict(model.named_parameters())).render(\"yoco_arch\", format=\"png\")\\n'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Training\n",
    "\n",
    "### Dataset Split\n",
    "\n",
    "The dataset was split into **80% training** and **20% validation**.\n",
    "Since synthetic data generation is no longer a bottleneck, this ratio was chosen for convenience and has **little practical impact on final performance**.\n",
    "\n",
    "\n",
    "### Optimization\n",
    "we used the default choice : Adam\n",
    "- **Learning Rate:** 1e-3\n",
    "  - Higher values were unstable.\n",
    "  - Lower values (10⁻4) led to extremely slow convergence.\n",
    "\n",
    "| Parameter     | Value                  |\n",
    "|---------------|------------------------|\n",
    "| Batch Size    | 16 (GPU), 1 (CPU)      |\n",
    "| Epochs        | 50–100                 |\n",
    "| Learning Rate | 1e-3                   |\n",
    "| Optimizer     | Adam                   |\n",
    "| Scheduler     | None                   |\n",
    "\n",
    "\n",
    "### Training Loss Plot\n",
    "REDO WITH BOTH TRAINING AND VALIDATION + F1\n",
    "<img src=\"src/loss.jpeg\" width=\"600\" height=\"400\"/>\n"
   ],
   "id": "10723580990b7599"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Evaluation\n",
    "\n",
    "### Custom F1-Score Metric\n",
    "\n",
    "To evaluate the performance of our YOCO (You Only Count Once) model, we where provided with a custom **F1-score** metric tailored for multi-class object counting.\n",
    "\n",
    "### Evaluation Process\n",
    "\n",
    "During training, evaluation was conducted on the **synthetic validation set**, which is structurally similar to the training images but randomly generated. However, to verify the generalization of the model, we also computed the custom F1-score on the **real training images**. This helped test whether the model overfit to synthetic artifacts or learned transferable features.\n",
    "\n",
    "### Quantitative Results\n",
    "\n",
    "**F1 Scores per Class:**\n",
    "\n",
    "| Class              | F1 Score |\n",
    "|--------------------|----------|\n",
    "| Jelly_White        | 0.8073   |\n",
    "| Jelly_Milk         | 0.9487   |\n",
    "| Jelly_Black        | 0.8602   |\n",
    "| Amandina           | 0.9315   |\n",
    "| Crème_brulée       | 0.9750   |\n",
    "| Triangolo          | 1.0000   |\n",
    "| Tentation_noir     | 0.9892   |\n",
    "| Comtesse           | 0.9204   |\n",
    "| Noblesse           | 1.0000   |\n",
    "| Noir_authentique   | 1.0000   |\n",
    "| Passion_au_lait    | 1.0000   |\n",
    "| Arabia             | 0.9897   |\n",
    "| Stracciatella      | 1.0000   |\n",
    "\n",
    "**Global F1 Score:** **0.9555**\n",
    "\n",
    "> The lower F1 scores observed for the *Jelly* series may be due to their **reflective surfaces**, which cause inconsistent appearances under lighting variation. We hypothesize that modifying the synthetic generator to include **color jittering or reflective noise simulation** could improve performance on these classes. Comptesse is also significantly worse than other chocolate (it's big white round chocolate.) an hypothesis is the difficulty to differentiate from the background and that it tend to see it where there are none (white background).\n",
    "\n",
    "### Visual Evaluation\n",
    "\n",
    "Visual comparison between predictions and ground truth on real images was performed manually. However, due to the counting-only nature of the model and absence of bounding boxes, typical detection visualizations (e.g., masks or heatmaps) are not applicable. The results confirmed that the model is **visually accurate** in object presence and counts, particularly on well-lit and non-reflective samples.\n"
   ],
   "id": "51f408c232718136"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Inference on Original Images & Result\n",
    "We made a script that load a checkpoint and run inference on the testing dataset and format the result in a CSV for kaggle. It also recompute the F1 score per class using the real image from the training dataset + provided CSV.\n",
    "\n",
    "- Show per-image table: `image ID | GT counts | Predicted counts | F1`\n",
    "\n",
    "- Final F1 score on original dataset.\n",
    "- Example success cases (model does great).\n",
    "- Example failure cases (too cluttered, occlusions, etc.)\n",
    "- Insights about how well model generalizes to real scenes.\n"
   ],
   "id": "a4e5f47594424ba5"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Discussion & Limitations\n",
    "- What worked well (e.g., synthetic scene generation).\n",
    "- What didn’t (e.g., failure on specific chocolate classes?).\n",
    "- Limitations of training from scratch.\n",
    "- Ideas for future work (e.g., more complex scene synthesis, weak supervision, semi-supervised learning).\n"
   ],
   "id": "d882d2decb42ce6c"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Appendix\n",
    "\n",
    "### Development Log – What We Tried\n",
    "\n",
    "This project went through several phases of experimentation. Here's a chronological overview of the different approaches we explored, with some thoughts on each one:\n",
    "\n",
    "1. **Autoencoder**\n",
    "   Our first (slightly naïve) attempt was to use an autoencoder, without fully understanding how it might work in the context of object counting. The idea was to reconstruct the image and spot anomalies that might correspond to pralines. Unsurprisingly, this didn’t yield meaningful results.\n",
    "\n",
    "2. **Ultralytics YOLO**\n",
    "   We then tried something more out-of-the-box: **Ultralytics YOLO**, using a pretrained model fine-tuned on our data. While promising at first, we realised later on that it was not allowed due to requiring the Ultralytics package.\n",
    "\n",
    "3. **Classical Machine Learning**\n",
    "   We also explored traditional ML methods (like regressions, k-NN, and random forests) using handcrafted features from the images. As expected, without  feature extraction or spatial context and due to skill issue, the models struggled to generalize and offered only limited performance. We did find a stupid heuristic that almost reach the baseline but the real world usage of such a solution remain unclear.\n",
    "\n",
    "4. **YOLOv1 Reimplementation in PyTorch**\n",
    "   To get a better grasp of the detection pipeline, we reimplemented **YOLOv1 from scratch in PyTorch**. This helped us understand how detection and classification interact, and gave us more control over the architecture and training process.\n",
    "\n",
    "5. **YOCO – “You Only Count Once” (Custom Model)**\n",
    "   Building on our previous attempts, we designed our own architecture specifically for **multi-class counting without localization**. Our model, YOCO, features a custom prediction head that outputs, for each class, a distribution over 6 count bins (0 to 5+).\n",
    "   Although inspired by YOLO in principle (for the convolution part), this model was built entirely from scratch and tailored to our needs: high-resolution image input, accurate per-class counts, and no bounding boxes.\n"
   ],
   "id": "56ec9fa583f3c52b"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Bonus\n",
    "\n",
    "Although we competed for the ML challenge, we also came up with a simple solution for the classical challenge by doing simple statistics on the training label only.\n",
    "We made a script to find the \"universal\" answer that would yield the highest F1 score in O(1) time and thus managed to reach F1 of ~0.4 by always predictive 1 for the number of instance for the 13 class. Although of little practical use we found it original, funny and stupid enough to deserve a mention here."
   ],
   "id": "770f47e4b42ca1aa"
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
